import cv2
from sklearn.cluster import KMeans
from sklearn import svm
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import plot_confusion_matrix
from scipy.cluster.vq import vq
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import datetime
import pickle

# define some constants
CURRENT_PATH = os.getcwd()
print(CURRENT_PATH)
#The data can be downloaded from src google-images-download, zip of 571M cannot be uploaded
#to the tencent platform with a restriction of 20M
IMG_PATH = os.path.join(CURRENT_PATH, 'data/images/')
IMG_LABELS_PATH = os.path.join(CURRENT_PATH, 'data/label_all.csv')
IF_RANDOM_SPLIT = False
IF_RUN_CAL_ALL_DESCRIPTORS = True
IF_RUN_CAL_CODEBOOK = True
IF_CAL_EACH_FEATURES = True
IF_TUNE_SVM = True
#IF_TRAIN_SVM = True
#IF_TUNE_KMEANS = False

def train_test_idx(all_img_idx, train_percent, if_random_split=IF_RANDOM_SPLIT):
    """
    :param all_img_idx: list of image index integers
    :param train_percent:
    :return:
    """
    num_train = int(len(all_img_idx) * train_percent)
    if if_random_split:
        np.random.seed(1)
        train_idx = np.random.choice(all_img_idx, num_train, replace=False)
        train_idx.sort()
        test_idx = np.setdiff1d(all_img_idx, train_idx)
    else:
        train_idx = all_img_idx[0: num_train]
        test_idx = all_img_idx[num_train:]
    print('train_idx: {}'.format(len(train_idx)))
    print('test_idx: {}'.format(len(test_idx)))
    return train_idx, test_idx

def cal_SIFT_all(train_idx):
    print('\nCalculating SIFT for all images:')
    descriptors_list = []
    for idx in train_idx:
        if idx % 100 == 0:
            print(idx)
        try:
            img_path = IMG_PATH + 'img_{}.jpg'.format(str(idx).zfill(3))
            image = cv2.imread(img_path, 0)

            sift = cv2.xfeatures2d.SIFT_create()
            kp, des = sift.detectAndCompute(image, None)

            # if des is not None:
            descriptors_list.append(des)  # descriptors = np.vstack((descriptors, des))
            # this_img_idx = np.array([idx + 1] * des.shape[0]).reshape((-1, 1))
            # img_idx_list.append(this_img_idx)  # img_idx = np.vstack((img_idx, this_img_idx))
        except cv2.error as e:
            print('Image {} error! '.format(idx), e)
    descriptors = np.concatenate(descriptors_list, axis=0)
    print('descriptors.shape: {}'.format(descriptors.shape))
    print('Calculating SIFT for all images completed!')
    return descriptors

def cal_codebook(all_descriptors, kmeans_model, num_clusters=40):
    all_descriptors = all_descriptors.astype(np.float32)

    # Apply KMeans
    if kmeans_model == 'cv2':
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)
        # Set flags (Just to avoid line break in the code)
        flags = cv2.KMEANS_RANDOM_CENTERS
        print('\nApplying cv2.kmeans:')
        start = datetime.datetime.now()
        compactness, labels, centers = cv2.kmeans(all_descriptors, num_clusters, None, criteria, 10, flags)
        print(labels.shape)
        #print(labels)
        # print(centers)
        end = datetime.datetime.now()
        elapsed_time = end-start
        print('kmeans elapsed time is: {}'.format(elapsed_time))
        print('cv2.kmeans completed!')
        return centers

    elif kmeans_model == 'sklearn':
        print('\nApplying sklearn.KMeans:')
        start = datetime.datetime.now()
        kmeans = KMeans(n_clusters=num_clusters, random_state=0, n_jobs=-2)
        kmeans.fit(all_descriptors)
        # print(kmeans.labels_)
        # print(kmeans.cluster_centers_)
        print('sse is {}'.format(kmeans.inertia_))
        end = datetime.datetime.now()
        print('time of running kmeans: {}'.format(end-start))
        print('skleran.KMeans completed!')
        print(type(kmeans.cluster_centers_))
        print(len(kmeans.cluster_centers_))
        print(kmeans.cluster_centers_.shape)
        return kmeans.cluster_centers_

def cal_img_features(img, codebook):
    """
    Calculate the features of a single image given the codebook(vocabulary) generated by clustering method (kmeans), each column is the center of the cluster.
    :param img:
    :param codebook:
    :return:
    """
    features = np.zeros((1, codebook.shape[1]))
    sift = cv2.xfeatures2d.SIFT_create()
    kp, des = sift.detectAndCompute(img, None)
    code, _ = vq(des, codebook)
    for i in code:
        features[0, i] += 1
    return(features)

def cal_img_features_all(img_idx, codebook):
    print('\nStart calculating all image features:')
    features_all_list = []
    for idx in img_idx:
        img_path = IMG_PATH + 'img_{}.jpg'.format(str(idx).zfill(3))
        image = cv2.imread(img_path, 0)
        this_features = cal_img_features(image, codebook)
        features_all_list.append(this_features)
    features_all = np.concatenate(features_all_list, axis=0)
    print('features all shape is: {}'.format(features_all.shape))
    print('Calculating all image features completed!')
    return features_all

def get_df_labels(img_idx):
    # df_labels = pd.read_csv(IMG_LABELS_PATH)
    # df_labels = np.array(df_labels.iloc[img_idx, 1]).reshape((-1,1))
    # return(df_labels)
    df_labels = pd.read_csv(IMG_LABELS_PATH)
    df_labels = df_labels[df_labels['Image'].isin(img_idx)]
    df_labels = np.array(df_labels.iloc[:, 1]).reshape((-1, 1))
    return df_labels

def get_processed_df(img_idx, codebook):
    """
    Get processed dataframe for modeling.
    :param img_idx: train中所有的照片的id
    :param codebook:
    :return:
    """
    features_all = cal_img_features_all(img_idx, codebook)
    df_labels = get_df_labels(img_idx)

    print('df_labels.shape: {}'.format(df_labels.shape))
    print('features_all.shape: {}'.format(features_all.shape))
    df = np.concatenate((features_all, df_labels), axis=1)
    #print(df)
    return df

def train_svm(train_df, C=10, if_tune=IF_TUNE_SVM):
    print('\nStart training SVM:')
    if if_tune:
        print('Tuning SVM:')
        X_train = train_df[:, :-1]
        y_train = train_df[:, -1]

        classifier = svm.SVC(kernel='linear')
        svc_param_grid = {'C': [0.01, 0.1, 1, 10, 100, 200, 500]}
        gsSVM = GridSearchCV(classifier, param_grid=svc_param_grid, scoring="accuracy", n_jobs=4, verbose=1)
        classifier = gsSVM.fit(X_train, y_train)
        print("svm:")
        print(X_train.shape, y_train.shape)
        print('grid search best score is: {}'.format(gsSVM.best_score_))
        print('grid search best model is: {}'.format(gsSVM.best_estimator_))
        # SVM.fit(X, Y)
        print('Tuning SVM completed!')
        return classifier
    else:
        X_train = train_df[:, :-1]
        y_train = train_df[:, -1]
        classifier = svm.SVC(kernel='linear', C=0.01).fit(X_train, y_train)
        np.set_printoptions(precision=2)
        return classifier
    print('Training SVM completed!')

def predict_svm(model, test_df, test_idx):
    print('\nStart predicting:')
    SVM = model
    X_test = test_df[:, :-1]
    y_test = test_df[:, -1]
    Y_pred = SVM.predict(X_test)

    final = pd.DataFrame({'Id': test_idx, 'TrueLabel':y_test, 'PredLabel': Y_pred})
    print(final['PredLabel'])
    final.to_csv('svm_1.csv', index=False)

    print(Y_pred.shape, y_test.shape, X_test.shape, test_df.shape, type(test_df))
    accuracy = sum(Y_pred == y_test)/len(y_test)
    print('accuracy is: {}'.format(accuracy))
    print('Predicting completed!')
    return accuracy

if __name__ == '__main__':
    class_names = ['forceps1', 'scissors1', 'scissors2', 'tweezers']
    all_img_idx = list(pd.read_csv(IMG_LABELS_PATH)['Image'])[:520]

    # Choose train and test dataset randomly or not.
    train_idx, test_idx = train_test_idx(all_img_idx, 0.6)

    # Calculate all the descriptors from train data using SIFT.
    if IF_RUN_CAL_ALL_DESCRIPTORS:
        all_descriptors = cal_SIFT_all(train_idx)

    # Applying sklearn.cluster.KMeans to all features.
    if IF_RUN_CAL_CODEBOOK:
        codebook = cal_codebook(all_descriptors, 'sklearn', 40)
        with open('./output/codebook', 'wb') as f:
            pickle.dump(codebook, f)
    else:
        with open('./output/codebook', 'rb') as f:
            codebook = pickle.load(f)

    # Respectively calculate features for train and test images.
    train_df = get_processed_df(train_idx, codebook)
    test_df = get_processed_df(test_idx, codebook)

    # Train svm classifier.
    classifier = train_svm(train_df)
    np.set_printoptions(precision=2)

    # Predict labels using the trained svm.
    predict_svm(classifier, test_df, test_idx)

    # Plot non-normalized confusion matrix
    titles_options = [("Confusion matrix, without normalization", None),
                      ("Normalized confusion matrix", 'true')]
    for title, normalize in titles_options:
        disp = plot_confusion_matrix(classifier, test_df[:, :-1], test_df[:, -1],
                                     display_labels=class_names,
                                     cmap=plt.cm.Blues,
                                     normalize=normalize)
        disp.ax_.set_title(title)
        print(title)
        print(disp.confusion_matrix)
        plt.show()


